# 计算社会学 报告

小组人数:3
林群_191250085_191250085@smail.nju.edu.cn
颜永健_191250172_191250172@smail.nju.edu.cn
罗庆滨_191250101_191250101@smail.nju.edu.cn
分工:
数据爬取，数据整理：林群
数据处理，心态分析：颜永健
可视化设计：罗庆滨

## 研究背景与目的

2019年12月下旬，中华人民共和国湖北省武汉市陆续有不明原因肺炎病例出现，其中有传言称部分病例经过诊断疑似为严重急性呼吸系统综合症病毒（下文简称SARS）感染，引发国内舆论对于SARS可能卷土重来的恐慌。2020年1月中旬，相关科研机构经过研究，证明是次疫情系一种全新的冠状病毒所导致的非典型肺炎综合症（以下简称“新冠肺炎”）。1月中旬起，中国境内的新冠肺炎患者确诊人数持续增加，证明新冠肺炎在中国境内开始成规模传播并且蔓延。因新冠肺炎对国民的生命健康的威胁极大，为了处置新冠肺炎疫情，全国各地陆续启动紧急响应以防控新冠肺炎疫情，而国内舆论也由于是次疫情防控工作的大规模而集中到了新冠疫情之上。

在这般严峻的社会背景与集中的舆论背景下，舆论的侧重点和舆论的心态投影的研究价值也便得以凸显。对于新闻工作者而言，他们在撰写媒体文章的时候，在新冠疫情不同的时期背景下倾向于强调哪些内容，又在新闻撰写的显性或者隐性情感传递中倾向于传达一种什么情感或者情感期望；对于广大的网民而言，他们在互联网社群以文字形式进行讨论时较为重视什么内容，又倾向于在讨论中传递怎样的情感，这些都值得进行一定的统计社会学研究，以总结是次新型冠状病毒肺炎疫情在我国的舆论表现情况，以及为持续性重大公共安全事故的舆论重点倾向研究以及舆论情感极性研究提供研究经验乃至于具合理性与可操作性的研究方案。

## 研究简介

爬取2019年12月31日至2020年6月15日之间，各大传统媒体（例：新华网，财新网，观察者网，新京报等）的新冠疫情议题报道的标题以及新闻正文内容，以及同时期关于新冠疫情的新闻（来源于上述的传统媒体以及其在社群网站的官方认证账户）的网民评论讨论，将总数据进行切词处理后人工构建停用词词典，根据停用词词典，而后根据停用词词典，将原始数据根据产生时间（以日为最小单位）分成四个不同阶段，进行切词处理后，计算四个阶段的TF_IDF数值；另根据总数据词频提取出正面心态极性和负面心态极性种子词，以原始数据为语料库进行心态词训练之后人工构建心态极性数值词典，以心态极性数值词典为依据，计算每日的心态极性数值统计量。而后对不同阶段的TF_IDF数值，以及以天为自变量的心态极性数值统计量的变化情况进行可视化处理之后，进行统计社会学视角分析，总结出不同阶段下新闻工作者和社群网站服务用户的信息关注倾向，以及各自的心态的变化情况及其可能的背后逻辑成因。

## 数据获取

考虑到数据集的全面性，官方的新闻更具有权威性，微博往往能吸引更多的评论，这里选取了新浪新闻的滚动新闻以及一些新浪微博大V发博作为数据源。选取新浪新闻的滚动新闻作为数据源的优点，一方面是滚动新闻能追溯到较早时间的新闻，另一方面新浪新闻的新闻来源比较丰富，既有官方网站的新闻，也有一些地方的新闻，也包含少量自媒体创作的文章，这些优点可以满足数据集的需求。而新浪微博拥有丰富的网友评论，可以一定程度上反映民众对新闻事件的看法，数据集中的评论部分主要由其构成，因而也是数据集构成不可少的部分。

在新闻方面，本小组选择了新浪新闻滚动新闻作为数据源；而在用户评论方面，本小组选择了央视新闻、人民日报、光明网、新华网、中国新闻网的官方新浪微博账号作为数据源。数据源的API以附录形式呈现。

### 代码实现

#### 新浪新闻爬取

由于新浪新闻的滚动新闻页是动态加载的，所以使用了`Python`的`Selenium`包，通过模拟浏览器加载从而获取网页信息。在对新闻进行筛选时，通过检索新闻标题，找到标题中包含关键字**疫**、**肺炎**、**病毒**的新闻并将其提取（我们认为包含以上关键字的新闻可以涵盖大部分有关疫情的新闻）:

```python
def get_page_news():
    news = browser.find_elements_by_xpath('//div[@class="d_list_txt"]/ul/li/span/a')
    page_news = []
    for i in news:
        link = i.get_attribute('href')
        res = requests.get(link)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')
        if len(soup.select('.second-title')) > 0:
            string = soup.select('.second-title')[0].text
        if '疫' in string or '肺炎' in string or '病毒' in string:
            page_news.append(get_news_content(link, soup))
    return page_news
```

#### 微博数据的爬取

考虑到微博数据量十分庞大，且微博带有反爬取措施，我们选择对爬取数据较为容易的微博移动端（https://weibo.cn)进行数据爬取。选取了需要爬取的微博用户的主页后，我们筛选出指定日期段内的微博，并对其中标题带有**疫**、**肺炎**、**病毒**、**新冠**的微博进行提取（同样认为以上关键词可以涵盖大部分有关疫情的微博）:

```python
def get_page_news(newsurl):
    data = []
    response = requests.get(newsurl, headers=headers)
    pattern = re.compile('<span class="ctt">.*?<span class="ct">.*?</span>', re.S)
    items = re.findall(pattern, response.text)
    for i in items:
        if '【' in i:
            title = get_title(i)
            if '疫' in title or '肺炎' in title or '病毒' in title or '新冠' in title:
                print(i)
                data.append(get_news_info(title, i))
    return data
```

之后通过正则表达式提取需要的信息：

```python
def get_news_info(title, news):
    article = {'title': title}
    pattern = re.compile('】.*?</span>')
    content = re.findall(pattern, news)[0].lstrip('】')
    article['content'] = remove_links(content)
    pattern = re.compile("赞.*?]")
    article['总点赞'] = re.findall(pattern,news)[0].lstrip('赞[').rstrip(']')
    pattern = re.compile("评论.*?]")
    article['总评论'] = re.findall(pattern,news)[0].lstrip('评论[').rstrip(']')
    pattern = re.compile('<span class="ct">.*?</span>')
    article['date'] = re.findall(pattern,news)[0].lstrip('<span class="ct">')[0:10]
    pattern = re.compile('attitude/.*?/')
    newsid = re.findall(pattern, news)[0].lstrip('attitude/').rstrip('/')
    try:
        article['comments'] = get_comments(newsid)
    except requests.exceptions.SSLError:
        article['comments'] = []
    return article
```

### 数据保存形式

爬取下来的数据以`.json`文件保存，对于新浪新闻的数据，主要保存的信息有：新闻来源链接（url），新闻标题，新闻来源网站，新闻日期，新闻内容和新闻前3条热度较高的评论。每则新闻的数据结构如下：

```json
{
        "comments": [],
        "content": "",
        "date": "",
        "source": "",
        "title": "",
        "url": ""
    }
```

对于新浪微博爬取的数据，主要保存的信息有：微博发布的日期，微博标题，微博的内容，微博总点赞数，微博总评论数，微博热门评论前5条。每条微博的数据结构如下：

```json
{
        "comments": [],
        "content": "",
        "date": "",
        "title": "",
        "总点赞": "",
        "总评论": ""
    }
```

## 数据处理与特征提炼

在介绍数据处理操作之前，进行以下名词的预定义：

- 停用词词典：以数据的预处理得到的词频结果以及人工的以数据研究价值为基准的基础上构建的，在数据处理过程中被排除的信息构成的数据结构。停用词词典中的信息不会参与后续的特征提炼。
- 原始数据：“数据获取”过程中得到的原始数据。
- 研究对象：对于原始数据进行预处理以及停用词筛选后得到的，用于后续一系列数值统计操作的数据结构
- 研究对象的X文件：研究对象中的类型为X的数据的最小构成单位。在新闻操作中为一则新闻，在评论中为一条评论。
- 心态极性数值：将词汇的心态分为正负（语言学意义上的乐观与悲观）两种极性倾向后，根据极性表现的显著程度对词汇赋予的数值。极性表现倾向越明显，且心态表现越极端，心态极性数值的绝对值越大；积极词汇的心态极性数值为正，消极词汇的心态极性数值为负。
- 心态字典：包含“词汇——心态极性数值”的映射关系的数据结构。
- 心态指数：根据以心态极性数值构建的心态字典，构建的心态极性数值统计量，以天为单位的形式呈现。

### 数据预处理

对于本次研究任务我们小组所获得到的数据，我们希望通过各类分析方法，从原始数据中提取到以下信息：

- 原始数据的随时间变化的阶段性特征，拟采用tf-idf算法获取关键词；
- 原始数据所反映的信息产生者（对于新闻信息为信息工作者，对于评论为评论者或者说广大网络用户）总体的心态变化，拟采用心态指数，通过心态极性数值拟合的方法将抽象的信息产生者心态极性数值化后，以时间（暂定以天为单位）作为自变量进行可视化，以直观反映信息产生者的心态变化

为获取以上信息，需要对原始数据进行预处理并且清洗无关数据。本小组以停用词词典以及预定义的冗余数据作为为依据进行数据清洗，其中，停用词词典主要包含以下类型的词汇：

- （新闻信息限定）所有的非汉字信息，因其在新闻媒体中绝大多数仅有基本的信息传递功能
- 仅用于基本信息的传达的高频词汇，比如“肺炎”“武汉市”“日”“月”“年”
- 本身单用无实际语义的词汇，比如“的”“了”“得”“是”“由于”等

冗余数据包含以下信息：

- 所有特殊符号，包括换行符、制表符
- 所有标点符号

最终得到的初步处理的数据为以“词汇：词频”形式表示的键值对构成的数据结构，在机器层面由python语言的dict数据结构保存。下文将该数据结构称为“研究对象”，将得到的数据称为“原始数据”。另为方便后续研究，将该数据结构以“词汇:词频”的形式保存于文件中。

数据预处理的代码实现如下：

~~~python
def  readJson(fPath,target,types):
    #输入原始数据，保存路径以及预期所需类型（指“标题类”“正文类”等，详情见数据获取部分）），输出为提取出来的原始数据子集
    files = os.listdir(fPath)
    for f in files:
        f= open(fPath+f,encoding="utf-8").read()
        result = json.loads(f)
        output = list()
        for i in range(0,len(result)):
            filePath = open(target), mode='a', encoding='utf-8')
            for j in result[i][types]:
                comments.append(j)
                filePath.writelines(j + "\n")
                
def calSumTf(fPath,target):
    #输入readjson所得数据以及结果路径，以文件形式输出研究对象
        target = open(fPath,mode="w",encoding="utf-8")
        dicts = getTFDicsts(path)
        copys = dicts.copy()
        for k in copys.keys():
            if k in ("👍","🙏","💪","❤","❌","😂","🎉","🐊","😷","👌","😭","👏","🙃","🌹","✊") or not re.match('[^0-9A-Za-z\u4e00-\u9fa5]+',k):
               target.writelines(k+":"+str(dicts[k])+"\n")

def getTFDicsts(filepath):
    #输入readjson所得数据，返回研究对象
    strs = open(filepath,encoding="utf-8").read()
    dicts2 = stopWordCal(dict(pandas.Series(jieba.lcut(strs,use_paddle=True)).value_counts()))
    return dicts2

def stopWordCal(dicts):
    stopWordList = [line.strip() for line in open(stopWordPath, encoding="utf_8").readlines()]
    copy = dicts.copy()
    for i in copy.keys():
        if i  in stopWordList:
            del dicts[i]
    return dicts
~~~

### TF-IDF数据获取

我们通过对于数据使用tf-idf算法，计算权重值:
$$
rank(word) = tf(word) * idf(word) * 1000
$$
其中：
$$
tf(word) = \frac{n_{word}}{sum}
$$
其中n~word~表示词语word在研究对象的词频，sum表示研究对象的总词数，在算法上等于研究对象的数值部分的求和。
$$
idf(word) = log_k(\frac{N}{num+1})
$$
其中N表示研究对象文件总数，num则表示在该研究对象中出现有word的文件数。此处的k值根据需要确定，以应对tfidf原始算法（即idf的对数基底取10）中对于样本量的较大依赖的问题，以及调整词汇分布广度在数据特征中的权重。

在研究中，我们将研究对象根据时空顺序分割成了四个部分，分别映射现实中2019冠状病毒病中国大陆疫情在2019年12月31日至2020年6月12日中的四个连续的时间阶段。对于新闻媒体，由于小组成员倾向于认为，新闻媒体为传述信息会不可避免地以一定频率使用一词，词汇的总词频的代表性相对于词汇的分布广度而言较低，因而新闻媒体的idf值取的对数基底较小，并且对于小样本部分的对数基底设置相对其他阶段的idf参数更小，以应对低样本量下idf值缺乏区分度的问题。在本次研究中，第一阶段的idf k值取2，后续三个阶段则取5。对于评论，由于评论的文件数较大，采用过小的k值可能导致tf数据被idf覆盖从而导致权重值扭曲失真，加之小组成员认为词汇广度在评论中的重要性较高，因而此处k值设定为6。

研究对象的四个部分的划分依据如下，以下简称阶段i,其中i为第i个部分，区间为闭区间：

阶段1：2019年12月31日~2020年1月22日，以华南海鲜市场封锁至武汉市封城为界

阶段2：2020年1月23日~2020年2月13日，以武汉市封城至李文亮医生逝世为界

阶段3：2020年2月14日~2020年3月11日，以李文亮医生逝世至国务院表示本土疫情得到基本阻断为界

阶段4：2020年3月12日~2020年6月15日，本土疫情得到阻断后时期，可以抽象理解为我国的“后疫情时期”

TF-IDF的数据操作代码如下（进行适当修改以增强可读性）:

~~~Python
def tf_idf(fPath,target):
    #fPath:研究对象保存文件路径；target:tfidf数据保存路径
        sumPaper = open(fPath,encoding='utf-8')
        sumFre = sumPaper.readlines()
        for i in range(0,len(sumFre)):
            sumFre[i] = sumFre[i].replace("\n","")
            sumFre[i] = sumFre[i].split(":")
            sumFre[i][1] = int(sumFre[i][1])
        wordSum = sum(i[1] for i in sumFre)
        tf = list(range(len(sumFre)))
        for i in range(0,len(sumFre)):
            tf[i] = sumFre[i][1]/wordSum
        idf = idfCalculate(sumFre,filesPath,k)
        for i in range(0,len(sumFre)):
            sumFre[i][1] = 1000 * tf[i] * idf[i]
        sumFre.sort(key=takeSecond,reverse = True)
        tfidf = open(target,mode="w",encoding="utf-8")
        for i in range(0,len(sumFre)):
            tfidf.write(sumFre[i][0]+":"+str(sumFre[i][1])+"\n")
        

def idfCalculate(dicts,filesPath,k):
    #filesPath为指定的文件化研究对象的父路径，文件化研究对象的文件生成使用readJson的改编方法，本份研究中不赘述;k为idf值的对数基底
    #文件化研究对象采二级存储结构，也即“父文件夹——子文件夹（命名为日期数据，以对应以天为单位研究心态极性指数）——文件名”
    j = 0
    fileList = os.listdir(filesPath)
    for i in range(0,len(dicts)):
        dicts[i][1] = 0
    for i in fileList:
        tempPath = path + i
        strs = open(tempPath,encoding="utf-8").read()
        dicts2 = dict(pandas.Series(jieba.lcut(strs,use_paddle=True)).value_counts())
        for i in range(0,len(dicts2)):
            if dicts[i][0] in dicts2.keys():
                dicts[i][1]+=1
        j+=1
    idf = list(range(len(dicts)))
    for i in range(0,len(dicts)):
        idf[i] = float(math.log((j)/(dicts[i][1]+1),k))
    return idf

~~~

### 心态获取

本小组通过构建数值化心态字典，并将研究对象与心态字典构建一对一的映射之后，将研究对象视为样本，计算样本的心态指数。本次研究中，样本以天为单位进行划分，心态指数则取心态极性数值的千倍样本一阶矩。

在对心态字典的构建方面，本小组先以人工标签的形式，对于研究对象中词频最高的2000个词汇进行筛选，获取到其中具有较强的情感倾向的词汇作为种子词，而后以研究对象作为语料库，通过SO_PMI算法辅以人工筛选，获得可用性较高的心态词，将这批心态词与种子词合并，进行数值化操作后即可获得数值化心态字典。

样本一阶矩通常可以较好的拟合总体分布的均值，在本次研究中，样本一阶矩可以一定程度上反映在本份研究的心态字典下，研究对象的心态画像，因而具备一定的统计学意义，同时也反映了其统计学意义决定于心态极性数值的构建合理性。

#### 心态字典的数值赋予原则

由于现有的计算机算法的局限性，由计算机自觉地获取合理的词汇情感极性具有较大难度乃至不现实性（实际上，在上文所述的心态词训练中，便出现了“同一个词汇在正面词训练结果中和负面词训练结果中均数值化表现出了较好的情感极性拟合度”，以及“某一情感极性的种子词反而更能拟合相反情感极性的训练结果”的现象），加之语料构成较为复杂，有必要在同一极性的词汇中进行程度区分，因而心态词典通过“人工对心态极性词赋值”的思路进行构建，不妨将此处的赋值称为“心态极性数值”，与作为研究对象提炼出的研究用的统计量“心态指数”做区分。

心态字典的心态极性数值赋值遵循下列基本原则：

- 不同词汇的心态极性数值的相对差异需合理
- 同意词汇的心态极性数值应统一

由于网络评论和典型新闻媒体报道的语料构成以及特性具有较大差异，因而对于两个数据的分析，需要构建两个不同的心态字典。

对于新闻报道，由于新闻报道构成较为复杂（包括事实信息的传递，可能的信息产生者的观点，以及信息产生者对于其他人的总结性描述等），因而在考虑其心态极性数值的赋予时，本小组从以下两个方向出发：

- 对于信息产生者（媒体）而言，使用这个词汇是否具有一定的情感传递目的以及动员倾向
- 对于信息接收者而言，其在接收这个词汇时是否会按照一定的情感倾向去理解

本份研究采信息接收者视角思路构建，并且在数据赋予时采取以下思路：

- 对于具有情感倾向，而出现频率偏高的词汇（主要集中于名词和动词），此类词汇往往会在一份样本中较多次重复出现，故适当下调其心态极性数值，以削弱重复所可能导致的重复计算导致的心态指数扭曲的情况。
- 新闻用词中存在部分词汇，其独立使用时不具有实际含义或者情感意义，但多用于表达某种特定心态或情感的场合，这类词汇同样赋予心态极性数值，同时由于此类词汇的可搭配性（其可能用于表达中性或者相反极性心态），心态极性数值累积结果可能存在偏差，因而适当调低心态极性数值以接近可能的真实情况
- 高词频词汇避免设定过高的权重以避免扭曲数据，降低数据的置信度
- 对于一个实义词汇，尤其动词和名词，如果其兼具信息传递与心态表达之功能，其心态极性数值的绝对值大小和该词汇所传递信息之重要性成正相关。举例说明，本份词典将“复工”“复产”“复学”等均列入积极极性词汇中并且赋予了较高的心态极性数值，是因为这些词汇传达的信息——我国社会活动恢复秩序——具有较高的重要性。

对于评论，则以“发送者角度”思考，即“评论信息的产生者在使用这个词汇时具有怎样的以及多大程度上的心态祈使动机”的角度考虑心态数值的设定，并采取以下思路：

- 部分词汇（以emoji为主）在表意的同时有极大的可能连续重复使用（典型如🙏），其心态极性数值需要适当调低
- 由于社群网络上文字信息使用具有极大的语法灵活性，构建心态词典时应当与词法文法脱钩，并且尽可能从口语表达角度去考虑词汇的极性（因评论大多情况下为口语化的文字表达）
- 避免将基本信息传递用词汇纳入心态词典中

构建完成的心态字典以“数据:心态极性数值绝对值”的形式保存于文档，通过读取每日的研究对象以及心态字典，即可计算该日的心态指数；所有心态指数计算完毕后保存于一份文档中。

心态指数计算的代码如下：

~~~python
def getExpre(posPath,negPath,dirPath,targetPath):
    #正向心态词典路径，负向心态词典路径，每日研究对象文件父路径，输出文件路径
        expDict = makeExDict(posPath,negPath)
        result = list()
        dirpath = "D:\文档\大二上\分条评论\\"
        paperList = os.listdir(dirpath)
        tempSum =0
        for j in range(0,len(paperList)):
            dirList = os.listdir(dirpath+paperList[j])
            result.append(
                paperList[j] + ":" )
            tempSum = 0
            for k in range(0,len(dirList)):
                tempSum+=float(calExpre(dirpath+
                                        paperList[j]+"\\"+dirList[k],expDict))
            result.append(str(tempSum) + "\n")
        open(target,mode = "w").writelines(result)

def makeExDict(posPath,negPath):
    expDict = dict()
    pos = open(posPath,encoding="utf-8").readlines()
    neg = open(negPath,encoding="utf-8").readlines()
    for i in range(0,len(pos)):
        pos[i] = pos[i].replace("\n","").split(" ")
        expDict[pos[i][0]] = float(pos[i][1])
    for i in range(0,len(neg)):
        neg[i] = neg[i].replace("\n","").split(" ")
        expDict[neg[i][0]] = - float(neg[i][1])
    return expDict

~~~

## 数据分析

在获取数据后，我们需要对数据进行可视化操作，并通过原始数据与可视化数据的观测，尝试对数据进行社会学角度的解释。

需要进行的数据分析操作包括：

- 对TF-IDF的阶段性数据的分析，提取不同阶段下新闻媒体和评论的各自的侧重信息
- 对以日为单位变化的心态指数进行分析，获取不同阶段下的心态指数大致情况以及其可能成因

### TF-IDF

阶段1：

新闻可视化词云：

![新闻阶段1](C:\Users\颜永健\Documents\Tencent Files\1466324326\FileRecv\新闻阶段1.png)

评论可视化词云：

![阶段_1](C:\Users\颜永健\Documents\Tencent Files\1466324326\FileRecv\阶段_1.png)

名词方面，口罩、N95、SARS、野味、病毒性表现出了较高的讨论权重，对应了当时社会的口罩高度短缺现象、新型冠状病毒勾起的人们对于2003年非典疫情的恐慌，以及对于可能导致病毒开始进入人类社会的野味食用现象的谴责。

此外，希望、好、平安、重视、能、加油等具有较高的积极鼓动性的词汇同样具有不菲的权重，这和当时国内舆论对于武汉的祈愿祝福关联。新闻报道方面的高权重词汇则集中于病理学词汇，这也反映了当时媒体对于神秘的新型冠状病毒的积极跟进。

总体而言，疫情带来的实际困境，疫情的现实话题，以及对于疫情结束的希望构成了阶段1评论的主旋律。

阶段2：

新闻可视化词云：

![新闻阶段2](C:\Users\颜永健\Documents\Tencent Files\1466324326\FileRecv\新闻阶段2.png)

影响和防控两词的高权重和当时社会背景息息相关——对于疫情的高压防控，以及高压防控下不可避免地我国正常社会生活秩序的破坏，构成了当时的主要社会背景。对于物资、口罩、捐赠、生产方面的高权重也反映了媒体对于疫情防控的后勤保障的高度重视，也和当时疫情防控物资的捉襟见肘挂钩。

评论可视化词云：

![阶段_2](C:\Users\颜永健\Documents\Tencent Files\1466324326\FileRecv\阶段_2.png)

口罩、加油、没有、好、能、希望等词的权重相比于阶段1更为突显，其他词汇的面积则显著偏小，反映出在社群网站上用户对于疫情对于现实生活的负面影响的发言，以及对于疫情过去的祈愿更为集中；相比于阶段1，阶段2的词云的词汇具有更多的实际意义不论大小，每个词汇过后，都是一个个与疫情息息相关的带有血泪的故事。其中，和疫情前线息息相关的词汇，以及对于疫情前线的患者和工作人员的祝福与鼓励占据了较高的权重。

阶段3：

新闻可视化词云：

![新闻阶段3](C:\Users\颜永健\Documents\Tencent Files\1466324326\FileRecv\新闻阶段3.png)

复工——社会生活逐步恢复正常秩序的标志——成为了这一阶段新闻媒体的最高权重词汇，反映了2月中下旬媒体对于社会生活恢复秩序的关切，以及三月上旬逐步有序复工以来媒体高频的跟踪报道。”恢复社会秩序“，不仅是当时媒体的主关注点，也是国内舆论的主关注点。

和疫情前线直接相关的词汇的权重大幅降低，反映了疫情前线紧张度的下跌以及相应的我国防控进入新阶段的现实。

相比于前两个阶段，物资相关的词汇权重大幅降低，也反映了我国医疗物资的社会生产力的恢复。

评论可视化词云：

![阶段_3](C:\Users\颜永健\Documents\Tencent Files\1466324326\FileRecv\阶段_3.png)

阶段3是我国疫情趋于好转、全球疫情出现恶化趋势的时间。在这样的时间背景下，国内的社群网站的讨论议题相应放宽，反映在词云上则为”居留“”永久“等词汇的权重的飙升（居留多出现于异地居住疑似病例或者境外相关报道中）。此外，阶段1阶段2的高权重的实际含义词权重大幅下跌，反映了疫情好转下疫情对我国社会生活的负面影响的下滑；祈愿式的词汇的高权重保持则照应了疫情仍在进行时的残酷现实。除了这些信息外，阶段3的词云大多延续了阶段2的鼓励前线的表达倾向，也照应了疫情仍在继续的现实。

阶段4：

新闻可视化词云：

![新闻阶段4](C:\Users\颜永健\Documents\Tencent Files\1466324326\FileRecv\新闻阶段4.png)

疫苗、检测、研究的权重的蹿升，前线相关的，意味着我们对于新冠肺炎的所知更多，以及相应的新冠肺炎疫情的长期性防控工作的转向。

评论可视化词云：

![阶段_4](C:\Users\颜永健\Documents\Tencent Files\1466324326\FileRecv\阶段_4.png)

美国——一个基本没有出现在前三个阶段的高权重词汇列表的词语——成为了阶段4的最高权重词汇，这和美国疫情的病毒式爆发蔓延，阶段4期间美国的消极抗疫，关于新冠病毒起源的美国阴谋论在国内的充分讨论以及国民长期以来的反美情绪存在关联。此外，开学、疫苗等词汇也出现在了高权重词汇列表中，反映了我国疫情防控进入了新阶段的现实。

### 心态指数

心态指数分析原始数据将以文件附录形式呈现，此处仅引用可视化结果。心态指数的分析需要分成新闻心态指数和评论心态指数分析。

可视化构建操作：用excel的自文本导入，以.txt:分隔得到日期-情感分析值表，将2019年12月31日设置为时间点“1”，并对每个日期赋予时间t值（部分日期内无数据，以此方法减小误差），再用Excel生成四个阶段下t与情感分析值的折线统计图。

总的心态指数变化折线图（以”新闻心态指数分析可视化结果“，”评论心态指数分析可视化结果“的顺序呈现，下同）：

![image-20210125223115616](C:\Users\颜永健\AppData\Roaming\Typora\typora-user-images\image-20210125223115616.png)

![image-20210125233329762](C:\Users\颜永健\AppData\Roaming\Typora\typora-user-images\image-20210125233329762.png)

对于新闻，整体而言，心态指数具有较为显著的不稳定性与波动性，这意味着使用函数拟合不再具有可行性；加之心态指数的阶段性特征明显，并且事实背景下新冠疫情的局势动荡使得其很难通过统计学方法予以预测和拟合，本小组最终采取了分阶段获取数据特征的思路。

对于评论，除了疫情高压期间的情感指数偏高以外整体趋于稳定，反映了大众对于疫情的积极性的期望。

阶段1：

![image-20210125223410113](C:\Users\颜永健\AppData\Roaming\Typora\typora-user-images\image-20210125223410113.png)

![image-20210125233622890](C:\Users\颜永健\AppData\Roaming\Typora\typora-user-images\image-20210125233622890.png)

阶段1对应的是疫情发展初期或者说爆发前，不难发现，该阶段的心态指数绝大多数时候较低，在疫情严重化（1月19日起）后的时期反而显著上升。此外，阶段1的心态指数在各阶段中是最低的，其均值显著低于零，也即新闻的整体心态极性是负值。对原始数据进行分析发现，早期的低心态指数可能和当时新冠病毒过分扑朔迷离导致的紧张感以及新冠病毒的过分未知性导致的撰文时的谨慎倾向，以及早期武汉市政府的过分低警惕存在关联。疫情爆发后的心态指数回升可能和当时社会的紧张感存在关联，新闻工作者有义务在恐慌情绪大规模散播的情况下，以调整报道情感偏向的方式调节社会情绪。

评论的变化趋势近似于新闻，反映了早期大众对于新冠疫情的负面情绪，以及随着负面情绪成为现实大众对于疫情结束的极为强烈的祈愿



阶段2：

![image-20210125224515526](C:\Users\颜永健\AppData\Roaming\Typora\typora-user-images\image-20210125224515526.png)

![image-20210125233810642](C:\Users\颜永健\AppData\Roaming\Typora\typora-user-images\image-20210125233810642.png)

阶段2的新闻心态指数整体趋于积极，但整体而言较低，尤其在2月上旬部分处于阶段一以来的较低点（也即阶段二的最低点）。阶段二初期的高心态指数与阶段一末期的高心态指数大致成因相同，都是严重社会事件背景下媒体工作者的有意识地新闻情绪调节；一月末至二月上旬一路走低的心态指数较大概率和当时背景下新冠疫情的负面影响（比如封城导致的社会问题，湖北省的医疗资源严重不足问题，早期疫情吹哨人的追踪调查，李文亮医生的病危等）迅速扩张存在关联。

评论和新闻的走势大致相同，一定程度上也反映了媒体对于大众的心态调节的实现。

阶段3：

![image-20210125225220361](C:\Users\颜永健\AppData\Roaming\Typora\typora-user-images\image-20210125225220361.png)

![image-20210125233913276](C:\Users\颜永健\AppData\Roaming\Typora\typora-user-images\image-20210125233913276.png)

同样处于疫情较为严重的时期，相比于阶段2，阶段3的心态指数整体而言较高。早期的低心态指数较大概率与李文亮医生之逝世存在对应关系，中间的低点则对应武汉市调整新冠确诊指标下的新冠病例激增的事件，可见媒体在重大社会负面舆情下仍然不可避免地在文章编写方面趋于负面。

评论的倾向则相对于新闻而言出现”整体偏高，相对偏低“的形势。

阶段4：

![image-20210125225520770](C:\Users\颜永健\AppData\Roaming\Typora\typora-user-images\image-20210125225520770.png)

![image-20210125234038325](C:\Users\颜永健\AppData\Roaming\Typora\typora-user-images\image-20210125234038325.png)

对应“后疫情时期”地阶段4整体而言心态指数较高，也反映了疫情基本告别，社会活动回复秩序后社会话题的积极转向；部分低点则或者和全球疫情爆发的关键时间契合，或与局部传输的本土疫情相关联。整体而言，媒体的心态指数和国内的本土疫情情况以及全球疫情发展情况对于我国的疫情防控的负面影响存在较强的相关性。

评论的心态一路走低，则较大概率和全球疫情的蔓延以及相应的国内小规模本土传播频率的增大存在关联。

## 总结

根据对TF-IDF以及心态指数的可视化数据的分析，本小组从中提炼出了一些情绪变化规律，并在和信息产生时间对应的现实背景的对比中，得出了一定的研究成果，也相应的检验了本小组的研究框架的可行性。

本次研究的研究小组由林群（组长），颜永健，罗庆滨负责，各同学的分工如下：

- 林群：研究方向的确定，数据获取工作及相应的文档撰写，数据可视化中TF-IDF词云的生成
- 颜永健：文档框架的起草，数据处理与特征提取工作及相应的文档撰写，数据的社会学分析
- 罗庆滨：数据可视化中心态指数的折线图的生成

### 研究价值

本小组讨论后所总结出的本份研究的价值如下：

- 提出了具一定统计学意义的对于舆论数据的数值分析方法流程，具有一定的可参考性以及复现性
- 根据信息来源的情绪表达特点针对性构建心态极性数值词典的操作以及操作结果，可以为文字数据的心态极性分析提供参考

### 研究局限

由于本次研究为几位在读本科生在各种各样的障碍下所进行的研究，会不可避免地出现一些研究局限性。其中有些局限性是不可控的，有些局限性则是存在优化空间的。本小组对于不同部分所反映出来的局限性进行总汇，并尝试性提供可能具可行性的研究优化方向。局限性类型在阐述开头以“（类型）”形式标记

数据获取部分：

- （不可控型）在过长的时间间隔下，部分数据可能失效而没能在本次研究中被爬取，可考虑使用互联网档案馆项目尝试获取失效新闻（可行性低）
- （不可控型）受限于国内舆论生态，有大量数据因为一定的舆论控制而被删除，未能被爬取，无优化可能性
- （可优化型）数据量较小，可增大数据爬取工作
- （可优化型）数据源较局限，可增大数据爬取工作

数据处理部分：

- （可优化型）数据预处理中的分词基于词典原理，所使用的默认词典不一定适合原始数据，可人工优化词典，或者采用其他算法尝试分词提高分词有效性（需一定数值分析基础，于研究人员当前学力可行性低）
- （可优化型）停用词词典较为主观且不完全，并且对于停用词的使用不充分，可改进停用词构建思路，并且提高停用词词典的构建操作
- （可优化型）心态词典构建较为依赖主观经验并且不完全，导致相应的心态数值乃至于心态指数存在失真可能性，可通过优化心态词典的构建，或者改用其他心态极性分析算法优化结果（需一定数值分析基础，于研究人员当前学力可行性低）
- （可优化型）心态指数的构建使用的经典的一阶矩，不一定能契合原样本，可尝试构建其他心态指数模型（需一定数值分析基础与统计学基础，于研究人员当前学力可行性低）

## 附录

#### 数据源

- 新浪新闻滚动新闻（2019-12-31~2020-06-15）：https://news.sina.com.cn/roll/#pageid=153&lid=2510&etime=1575734400&stime=1575820800&ctime=1575820800&date=2019-12-08&k=&num=50&page=1
- 新浪微博大V发博（2019-12-31~2020-06-15）：
  - 央视新闻：https://weibo.cn/2656274875/profile?hasori=0&haspic=0&starttime=20191208&endtime=20200615&advancedfilter=1&page=1
  - 人民日报：https://weibo.cn/2803301701/profile?hasori=0&haspic=0&starttime=20191208&endtime=20200615&advancedfilter=1&page=1
  - 光明网：https://weibo.cn/1752825395/profile?hasori=0&haspic=0&starttime=20191208&endtime=20200615&advancedfilter=1&page=1
  - 新华网：https://weibo.cn/1699432410/profile?hasori=0&haspic=0&starttime=20191208&endtime=20200615&advancedfilter=1&page=1
  - 中国新闻网：https://weibo.cn/1784473157/profile?hasori=0&haspic=0&starttime=20191208&endtime=20200615&advancedfilter=1&page=1

#### 项目使用

*注：以下项目引用格式统一为项目名称(项目译名):项目地址超链接，项目在本研究中的用途*

- Jieba(结巴分词):https://github.com/fxsjy/jieba，对原始数据进行预处理获得可操作的数据对象
- wordexpansion:https://github.com/thunderhit/wordexpansion，训练心态词典

#### 作业项目链接

https://github.com/Caserta-012/DataScience